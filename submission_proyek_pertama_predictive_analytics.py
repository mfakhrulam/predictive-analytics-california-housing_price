# -*- coding: utf-8 -*-
"""Submission_Proyek_Pertama_Predictive_Analytics.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zcRAB_9MTEZtBYjgZNKC887SV2ct_x5y

https://www.kaggle.com/datasets/camnugent/california-housing-prices/code

# Import Library
"""

# Commented out IPython magic to ensure Python compatibility.
from google.colab import files
import pandas as pd
import seaborn as sns
import tensorflow as tf
import matplotlib.pyplot as plt
# %matplotlib inline

# Data preparation
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler

# Modelling
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import mean_absolute_error
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import AdaBoostRegressor
from sklearn.neighbors import KNeighborsRegressor

"""# Sambungkan ke kaggle dan download dataset"""

! pip install -q kaggle
files.upload()
! mkdir ~/.kaggle
! cp kaggle.json ~/.kaggle/
! chmod 600 ~/.kaggle/kaggle.json

! kaggle datasets download -d camnugent/california-housing-prices -p /content/ --unzip

"""# Load data"""

housing = pd.read_csv('/content/housing.csv')
housing.head()

"""# Exploratory Data Analysis (Data Understanding)"""

housing.shape

housing.info()

housing.describe()

"""## Missing value



"""

housing.isnull().sum()

sns.heatmap(housing.isnull(), cbar = False, cmap='flare')

"""Ganti nilai na pada kolom "total_bedrooms" dengan mediannya



"""

print('Median fitur "total_bedrooms":', housing['total_bedrooms'].median())

housing['total_bedrooms'].fillna(value=housing['total_bedrooms'].median(), inplace=True)

housing.isna().sum()

"""## Outliers

"""

fig, axes = plt.subplots(3, 3, figsize=(18, 10))
fig.suptitle('Outlier')
#  longitude	latitude	housing_median_age	total_rooms	total_bedrooms	population	households	median_income median_house_value
sns.boxplot(ax=axes[0, 0], x=housing['longitude'])
sns.boxplot(ax=axes[0, 1], x=housing['latitude'])
sns.boxplot(ax=axes[0, 2], x=housing['housing_median_age'])
sns.boxplot(ax=axes[1, 0], x=housing['total_rooms'])
sns.boxplot(ax=axes[1, 1], x=housing['total_bedrooms'])
sns.boxplot(ax=axes[1, 2], x=housing['population'])
sns.boxplot(ax=axes[2, 0], x=housing['households'])
sns.boxplot(ax=axes[2, 1], x=housing['median_income'])
sns.boxplot(ax=axes[2, 2], x=housing['median_house_value'])

"""## Univariate Analysis"""

# Pembagian categorical dan numerical features
categorical_features = [x for x in housing.columns if housing[x].dtypes == 'object']
numerical_features = [x for x in housing.columns if x not in categorical_features]
print('Categorical :', categorical_features)
print('Numerical   :', numerical_features)

"""### Categorical Features

#### Fitur ocean_proximity
"""

feature = categorical_features[0]
count = housing[feature].value_counts()
percent = 100*housing[feature].value_counts(normalize=True)
df = pd.DataFrame({'jumlah sampel':count, 'persentase':percent.round(1)})
print(df)
count.plot(kind='bar', title=feature);

"""### Numerical Features"""

housing.hist(bins=50, figsize=(20,10))
plt.show()

"""## Multivariate Analysis

### Categorical Features
"""

for col in categorical_features:
  sns.catplot(x=col, y='median_house_value', kind="bar", dodge=False, height = 6, aspect=3, data=housing, palette="Set3")
  plt.title("Rata-rata 'median_house_value' relatif terhadap - {}".format(col))

"""### Numerical Features"""

# Mengamati hubungan antar fitur numerik dengan fungsi pairplot()
sns.pairplot(housing, diag_kind = 'kde')

"""Evaluasi skor korelasi fitur"""

plt.figure(figsize=(10, 8))
correlation_matrix = housing.corr().round(2)
 
# Untuk menge-print nilai di dalam kotak, gunakan parameter anot=True
sns.heatmap(data=correlation_matrix, annot=True, cmap='flare', linewidths=0.5, )
plt.title("Correlation Matrix untuk Fitur Numerik ", size=20)

"""## Kesimpulan EDA  
- Terdapat outlier di banyak fitur yang perlu dihilangkan
- Terdapat korelasi yang cukup tinggi antara median_house_value (target) dengan median_income
- Rumah paling banyak terletak pada <1H OCEAN
- Terdapat korelasi yang tinggi antara total_room, total_beedroom, population, dan households

# Data Preparation

## Menghilangkan Outlier
"""

# Hilangkan Outliers
Q1 = housing.quantile(0.25)
Q3 = housing.quantile(0.75)
IQR=Q3-Q1
housing=housing[~((housing<(Q1-1.5*IQR))|(housing>(Q3+1.5*IQR))).any(axis=1)]
housing.shape

"""## Encoding fitur kategori"""

housing = pd.concat([housing, pd.get_dummies(housing['ocean_proximity'], prefix='ocean_prox')],axis=1)
housing.drop(['ocean_proximity'], axis=1, inplace=True)
housing.head()

"""## Reduction dimensions

Jika dicek menggunakan fungsi pairplot, kedua fitur letak rumah dalam kolom longitude dan latitude memiliki korelasi yang tinggi. Hal ini karena kedua fitur ini memiliki informasi yang sama, yaitu letak/koordinat rumah.
"""

sns.pairplot(housing[['longitude','latitude']], plot_kws={"s": 2});

# aplikasikan PCA
pca = PCA(n_components=2, random_state=42)
pca.fit(housing[['longitude','latitude']])
princ_comp = pca.transform(housing[['longitude','latitude']])

pca.explained_variance_ratio_.round(3)

pca = PCA(n_components=1, random_state=42)
pca.fit(housing[['longitude','latitude']])
housing['coordinate'] = pca.transform(housing.loc[:, ('longitude','latitude')]).flatten()
housing.drop(['longitude','latitude'], axis=1, inplace=True)

housing.head()

"""## Train test split"""

X = housing.drop(["median_house_value"],axis =1)
y = housing["median_house_value"]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)

print(f'Total # of sample in whole dataset: {len(X)}')
print(f'Total # of sample in train dataset: {len(X_train)}')
print(f'Total # of sample in validation dataset: {len(X_test)}')

"""## Standarisasi"""

# housing_median_age	total_rooms	total_bedrooms	population	households	median_income coordinate
numerical_features = ['housing_median_age',
                      'total_rooms',
                      'total_bedrooms', 
                      'population',
                      'households',
                      'median_income',
                      'coordinate']
scaler = StandardScaler()
scaler.fit(X_train[numerical_features])
X_train[numerical_features] = scaler.transform(X_train.loc[:, numerical_features])
X_train[numerical_features].head()

X_train[numerical_features].describe().round(4)

"""# Model Development"""

# Siapkan dataframe untuk analisis model
models = pd.DataFrame(index=['train_mae', 'test_mae'], 
                      columns=['KNN', 'RandomForest', 'Boosting'])

"""## KNN"""

# Gunakan gridsearch untuk mencari hasil terbaik
neighbors = list(range(1,15))
parameters = {
    'n_neighbors':neighbors
}
knn = KNeighborsRegressor()
grid_search = GridSearchCV(knn, parameters, verbose=2)
grid_search.fit(X_train, y_train)
print("KNN GridSearch score: "+str(grid_search.best_score_))
print("KNN GridSearch params: "+str(grid_search.best_params_))

# buat model prediksi
knn = KNeighborsRegressor(n_neighbors=11)
knn.fit(X_train, y_train)

"""## Random Forest"""

# Gunakan gridsearch untuk mencari hasil terbaik
n_estimators = list(range(40,101,20))
max_depth = list(range(10,20,2))

parameters = {
    'n_estimators':n_estimators,
    'max_depth':max_depth
}
RF = RandomForestRegressor()
grid_search = GridSearchCV(RF, parameters, verbose=2)
grid_search.fit(X_train, y_train)
print("RF GridSearch score: "+str(grid_search.best_score_))
print("RF GridSearch params: "+str(grid_search.best_params_))

# buat model prediksi
RF = RandomForestRegressor(n_estimators=100, max_depth=18, random_state=42, n_jobs=-1)
RF.fit(X_train, y_train)

"""## Boosting Algorithm"""

# Gunakan gridsearch untuk mencari hasil terbaik
parameters = {
    'learning_rate':[0.1,0.3,0.5,0.05,0.005]
}
boosting = AdaBoostRegressor()
grid_search = GridSearchCV(boosting, parameters, verbose=2)
grid_search.fit(X_train, y_train)
print("Boosting GridSearch score: "+str(grid_search.best_score_))
print("Boosting GridSearch params: "+str(grid_search.best_params_))

# buat model prediksi
boosting = AdaBoostRegressor(learning_rate=0.05, random_state=42)                             
boosting.fit(X_train, y_train)

"""# Evaluasi Model"""

# Lakukan scaling terhadap fitur numerik pada X_test sehingga memiliki rata-rata=0 dan varians=1
X_test.loc[:, numerical_features] = scaler.transform(X_test[numerical_features])

# Buat variabel mae yang isinya adalah dataframe nilai mae data train dan test pada masing-masing algoritma
mae = pd.DataFrame(columns=['mae_train', 'mae_test'], index=['KNN','RF','Boosting'])
 
# Buat dictionary untuk setiap algoritma yang digunakan
model_dict = {'KNN': knn, 'RF': RF, 'Boosting': boosting}
 
# Hitung Mean Squared Error masing-masing algoritma pada data train dan test
for name, model in model_dict.items():
    mae.loc[name, 'mae_train'] = mean_absolute_error(y_true=y_train, y_pred=model.predict(X_train)) 
    mae.loc[name, 'mae_test'] = mean_absolute_error(y_true=y_test, y_pred=model.predict(X_test))

mae

mae_scale = abs(housing['median_house_value'].max()-housing['median_house_value'].min())*0.1
mae_scale

"""Tampak bahwa mae dari model KNN dan RF sudah di bawah 10% MAE yang telah di-scale berdasarkan data. Hal ini berarti model KNN dan RF sudah bagus, tetapi masih overfitting terhadap data training karena nilai MAE test masih lebih tinggi."""

fig, ax = plt.subplots()
mae.sort_values(by='mae_test', ascending=False).plot(kind='barh', ax=ax, zorder=3)
ax.grid(zorder=0)

prediksi = X_test.iloc[:].copy()
pred_dict = {'y_true':y_test[:]}
for name, model_pred in model_dict.items():
    pred_dict['prediksi_'+name] = model_pred.predict(prediksi).round(1)
 
pred = pd.DataFrame(pred_dict)

pred.head()

plt.figure(figsize=(8,15))
plt.subplot(311)
plt.title('KNN')
plt.scatter(pred['prediksi_KNN'], pred['y_true'])

plt.subplot(312)
plt.title('Random Forest')
plt.scatter(pred['prediksi_RF'], pred['y_true'])

plt.subplot(313)
plt.title('Boosting')
plt.scatter(pred['prediksi_Boosting'], pred['y_true'])
plt.show()